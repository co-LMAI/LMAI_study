{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-23T07:32:07.273376Z",
          "iopub.status.busy": "2025-03-23T07:32:07.273033Z",
          "iopub.status.idle": "2025-03-23T07:32:07.276347Z",
          "shell.execute_reply": "2025-03-23T07:32:07.275883Z",
          "shell.execute_reply.started": "2025-03-23T07:32:07.273354Z"
        },
        "id": "0-69dmxkAAoj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jieba\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2025-03-23T07:32:08.570415Z",
          "iopub.status.busy": "2025-03-23T07:32:08.570088Z",
          "iopub.status.idle": "2025-03-23T07:32:09.214886Z",
          "shell.execute_reply": "2025-03-23T07:32:09.214369Z",
          "shell.execute_reply.started": "2025-03-23T07:32:08.570394Z"
        },
        "id": "CXSJImdCAAok",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def read_data(file_path):\n",
        "    src = []\n",
        "    tgt = []\n",
        "    with open(file_path , mode='r',encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            s,t = line.strip().lower().split('\\t')\n",
        "            s = s.split(' ')\n",
        "            t = t.split(' ')\n",
        "            src.append(s)\n",
        "            tgt.append(t)\n",
        "    return src,tgt\n",
        "path = r'.\\data\\eng-fra.txt'\n",
        "src_data ,tgt_data =  read_data(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-23T07:32:13.933756Z",
          "iopub.status.busy": "2025-03-23T07:32:13.933443Z",
          "iopub.status.idle": "2025-03-23T07:32:13.936747Z",
          "shell.execute_reply": "2025-03-23T07:32:13.936335Z",
          "shell.execute_reply.started": "2025-03-23T07:32:13.933736Z"
        },
        "id": "1XvDetJ8AAol",
        "outputId": "adde78cc-3f02-425a-b4d1-0dca4ed2a6f5",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['wait!'], ['wait!'], ['i', 'see.'], ['i', 'try.'], ['i', 'won!'], ['i', 'won!'], ['oh', 'no!'], ['attack!'], ['attack!'], ['cheers!']]\n",
            "[['attends', '!'], ['attendez', '!'], ['je', 'comprends.'], [\"j'essaye.\"], [\"j'ai\", 'gagné', '!'], ['je', \"l'ai\", 'emporté', '!'], ['oh', 'non', '!'], ['attaque', '!'], ['attaquez', '!'], ['santé', '!']]\n"
          ]
        }
      ],
      "source": [
        "print(src_data[10:20])\n",
        "print(tgt_data[10:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-23T07:32:14.986728Z",
          "iopub.status.busy": "2025-03-23T07:32:14.986430Z",
          "iopub.status.idle": "2025-03-23T07:32:15.364258Z",
          "shell.execute_reply": "2025-03-23T07:32:15.363650Z",
          "shell.execute_reply.started": "2025-03-23T07:32:14.986705Z"
        },
        "id": "vb4yR0lfAAom",
        "outputId": "242d64aa-5abc-4c1b-bd33-ba13909bf73d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "he\n",
            "le\n"
          ]
        }
      ],
      "source": [
        "# 构建词汇表\n",
        "class Vocab:\n",
        "    '''词汇表类，用于从文本数据中构建词汇表'''\n",
        "    # 定义类属性\n",
        "    # 定义填充符号、未识别符号、开始符合、结束符号\n",
        "    PAD = '<pad>'\n",
        "    UNK = '<unk>'\n",
        "    SOS = '<SOS>'\n",
        "    EOS = '<EOS>'\n",
        "\n",
        "    pad_idx = 0\n",
        "    unk_idx = 1\n",
        "    sos_idx = 2\n",
        "    eos_idx = 3\n",
        "\n",
        "    def __init__(self,text,max_vocab = 5000):\n",
        "        '''初始化，构建词汇表\n",
        "        parameter\n",
        "        -----------------\n",
        "        text:array of list\n",
        "            包含文本的数据集\n",
        "        max_vocab :int\n",
        "            词汇表最大长度\n",
        "        '''\n",
        "        vocab = Counter()\n",
        "        for text_line in text:\n",
        "            vocab.update(text_line)\n",
        "\n",
        "        self.word_index = {}\n",
        "        c = self.__class__\n",
        "        d = {c.PAD:c.pad_idx,\n",
        "             c.UNK:c.unk_idx,\n",
        "             c.SOS:c.sos_idx,\n",
        "             c.EOS:c.eos_idx\n",
        "             }\n",
        "        self.word_index.update(d)\n",
        "\n",
        "        for idx ,(word,count) in enumerate(vocab.most_common(max_vocab-4),start = 4):\n",
        "            self.word_index[word]=idx\n",
        "\n",
        "        self.index_word = {index:word for word,index in self.word_index.items()}\n",
        "        self.vocab_size = len(self.word_index)\n",
        "\n",
        "src_vocab = Vocab(src_data)\n",
        "tgt_vocab = Vocab(tgt_data)\n",
        "print(src_vocab.index_word[10])\n",
        "print(tgt_vocab.index_word[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "5m17xVEUGXZI"
      },
      "outputs": [],
      "source": [
        "# 自定义数据集\n",
        "class ParallelDataset(Dataset):\n",
        "    '''自定义数据集类，获取平行数据'''\n",
        "    def __init__(self,src_data,tgt_data,src_vocab,tgt_vocab,\n",
        "                 max_src_length=None,max_tgt_length=None):\n",
        "        if max_src_length is None:\n",
        "             max_src_length = self.__get_max_seq_len__(src_data)\n",
        "        if max_tgt_length is None:\n",
        "             max_tgt_length = self.__get_max_seq_len__(tgt_data)\n",
        "\n",
        "        self.data = []\n",
        "        for src,tgt in zip(src_data,tgt_data):\n",
        "            src_idx = [src_vocab.word_index.get(token,Vocab.unk_idx) for token in src]\n",
        "            tgt_idx = [tgt_vocab.word_index.get(token,Vocab.unk_idx) for token in tgt]\n",
        "\n",
        "            tgt_idx = [Vocab.sos_idx] + tgt_idx + [Vocab.eos_idx]\n",
        "            # 填充或截断\n",
        "            src_idx = self.__pad_or_truncatr__(src_idx,max_src_length)\n",
        "            tgt_idx = self.__pad_or_truncatr__(tgt_idx,max_tgt_length)\n",
        "            # 将序列转化为张量\n",
        "            src_idx = torch.LongTensor(src_idx)\n",
        "            tgt_idx = torch.LongTensor(tgt_idx)\n",
        "\n",
        "            self.data.append((src_idx,tgt_idx))\n",
        "\n",
        "    def __len__(self):\n",
        "         return len(self.data)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "         return self.data[index]\n",
        "\n",
        "    def __pad_or_truncatr__(self,seq,max_len):\n",
        "        seq_len = len(seq)\n",
        "        if seq_len>max_len:\n",
        "            seq = seq[:max_len]\n",
        "        else:\n",
        "            seq = seq + [Vocab.pad_idx]*(max_len - seq_len)\n",
        "        return seq\n",
        "\n",
        "    def __get_max_seq_len__(self,text_data):\n",
        "            max_len = max(len(d) for d in text_data)\n",
        "            return max_len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2025-03-23T07:32:22.865993Z",
          "iopub.status.busy": "2025-03-23T07:32:22.865659Z",
          "iopub.status.idle": "2025-03-23T07:32:22.871156Z",
          "shell.execute_reply": "2025-03-23T07:32:22.870552Z",
          "shell.execute_reply.started": "2025-03-23T07:32:22.865971Z"
        },
        "id": "t2Q8RYyuAAop",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 位置编码\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,d_model,max_length = 1000):\n",
        "        '''初始化方法\n",
        "\n",
        "        parameter\n",
        "        --------------\n",
        "        d_model ：int\n",
        "            嵌入向量维度\n",
        "        max_length ：int\n",
        "            最大序列长度\n",
        "        '''\n",
        "        super().__init__()\n",
        "        # 创建位置编码矩阵\n",
        "        pe = torch.zeros(max_length,d_model)\n",
        "        # 创建一个一维张量，其元素为从0到max_length-1，便是序列中的各个位置\n",
        "        # 将形状转（max_length,)换为（max_length,1)，便于后续计算\n",
        "        position = torch.arange(0,max_length,dtype=torch.float).unsqueeze(1)\n",
        "        # exp(log(a)*b) = a^b\n",
        "\n",
        "        div_trem = torch.exp(torch.arange(0,d_model,2) * (-np.log(10000.0)/d_model))\n",
        "        # d_model必须为偶数，保证奇数长度与偶数长度相同\n",
        "        # Position*div_trem.shape = (max_length,d_model/2)\n",
        "        pe[:,0::2] = torch.sin(position * div_trem)\n",
        "        pe[:,1::2] = torch.cos(position * div_trem)\n",
        "        # 将pe注册为模型的缓冲区\n",
        "        # 缓冲区时pytorch中的一种特殊属性，其不会被计算图追踪，不会更新梯度\n",
        "        # 但是，成为缓冲区后，会成为state_dict的一部分，会随着模型一起保存和加载\n",
        "        # 当注册缓冲区后，变量就会绑定当前对象，成为当前对象属性\n",
        "        # 注册属性与绑定属性的区别:\n",
        "            # 1、缓冲区会随着模型一起保存和加载，但是绑定属性无此功能\n",
        "            # 2、缓冲区与模型参数一样，会随着模型一起迁移，但绑定属性无此功能\n",
        "        self.register_buffer('pe',pe)\n",
        "\n",
        "    def forward(self,x):\n",
        "        # x.shape = (batch_size,seq_length,d_model)\n",
        "        # 将词嵌入向量与位置张量相加\n",
        "        x += self.pe[:x.size(1)].to(x.device)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2025-03-23T07:35:50.508683Z",
          "iopub.status.busy": "2025-03-23T07:35:50.508149Z",
          "iopub.status.idle": "2025-03-23T07:35:50.515832Z",
          "shell.execute_reply": "2025-03-23T07:35:50.515326Z",
          "shell.execute_reply.started": "2025-03-23T07:35:50.508661Z"
        },
        "id": "kAR7ltDeAAoq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    '''transformer模型类\n",
        "    pytorch中提供的tansformer类,不包含词嵌入和位置编码以及输出层\n",
        "    '''\n",
        "    def __init__(self,scr_vocab_size,tgt_vocab_size,d_model = 512,\n",
        "                  num_heads = 8,num_layers = 6,ff_dim = 2048,dropout = 0.1):\n",
        "            super().__init__()\n",
        "            self.src_embed = nn.Embedding(scr_vocab_size,d_model)\n",
        "            self.tgt_embed = nn.Embedding(tgt_vocab_size,d_model)\n",
        "            self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "\n",
        "            # Transformer层\n",
        "            self.transformer = nn.Transformer(\n",
        "                d_model=d_model,\n",
        "                nhead=num_heads,\n",
        "                num_encoder_layers=num_layers,\n",
        "                num_decoder_layers=num_layers,\n",
        "                dim_feedforward=ff_dim,\n",
        "                dropout=dropout,\n",
        "                batch_first=True\n",
        "            )\n",
        "            # 输出层\n",
        "            self.fc_out = nn.Linear(d_model,tgt_vocab_size)\n",
        "\n",
        "    def forward(self,src_ids,tgt_ids):\n",
        "        '''\n",
        "        parameter\n",
        "        ---------------\n",
        "        src_ids : torch.tensor shape = (batch_size,seq_len)\n",
        "            输入源数据索引\n",
        "        tgt_ids : torch.tensr shape = (batcg_size,seq_len)\n",
        "            输入目标系列索引\n",
        "\n",
        "        return\n",
        "        --------------\n",
        "        out : torch.tensor shape = (batch_size,tgt_seq_len,tgt_vocab_size)\n",
        "        '''\n",
        "        src_mask,tgt_mask,src_pad_mask,tgt_pad_mask = self.generate_mask(src_ids,tgt_ids)\n",
        "        src_embed = self.pos_encoder(self.src_embed(src_ids))\n",
        "        tgt_embed = self.pos_encoder(self.tgt_embed(tgt_ids))\n",
        "        out = self.transformer(\n",
        "            src_embed,\n",
        "            tgt_embed,\n",
        "            src_mask,# 编码器自注意力掩码\n",
        "            tgt_mask,# 解码器器自注意力掩码\n",
        "            memory_mask = None,# 解码器参考编码器的掩码\n",
        "            src_key_padding_mask = src_pad_mask,# 对编码器中填充的词进行掩码\n",
        "            tgt_key_padding_mask = tgt_pad_mask,# 解码器中填充的词，进行掩码\n",
        "            memory_key_padding_mask = src_pad_mask # 解码器中，参考编码器中填充的掩码\n",
        "        )\n",
        "\n",
        "        return self.fc_out(out)\n",
        "\n",
        "\n",
        "    def generate_mask(self,src,tgt):\n",
        "        '''\n",
        "        生成编码器与解码器的注意力掩码与填充掩码\n",
        "        parameter\n",
        "        -----------------\n",
        "        src : torch.tensor shape = (batch_size,src_seq_len)\n",
        "            源序列\n",
        "        tgt : torch.tensor shape = (batch_size,tgt_seq_len)\n",
        "            目标序列\n",
        "\n",
        "        return\n",
        "        ------------------\n",
        "        src_mask: shape = (src_seq_len,src_seq_len)\n",
        "            编码器的注意力掩码\n",
        "        tgt_mask : shape =(tgt_seq_len,tgt_seq_len)\n",
        "            解码器的注意力掩码\n",
        "        src_pad_mask : shape = (batch_size,src_seq_len)\n",
        "            编码器的填充掩码\n",
        "        tgt_pad_mask : shape = (batch_size,tgt_seq_len)\n",
        "            解码器的填充掩码\n",
        "        '''\n",
        "        src_mask = torch.zeros(src.size(1), src.size(1)).to(src.device)\n",
        "        #tgt_mask = self.generate_causal_mask(tgt.size(1))\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
        "        src_pad_mask = (src == Vocab.pad_idx)\n",
        "        tgt_pad_mask = (tgt == Vocab.pad_idx)\n",
        "        return src_mask,tgt_mask,src_pad_mask,tgt_pad_mask\n",
        "\n",
        "\n",
        "    def generate_causal_mask(self,seq_len):\n",
        "        mask = torch.triu(torch.ones(seq_len,seq_len),diagonal=1)\n",
        "        return mask.bool()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "ExecutionIndicator": {
          "show": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.idle": "2025-03-23T07:50:41.717094Z",
          "shell.execute_reply": "2025-03-23T07:50:41.715798Z",
          "shell.execute_reply.started": "2025-03-23T07:35:53.630561Z"
        },
        "id": "vN-IEb__AAoq",
        "outputId": "91ad4011-ab88-47a9-f667-9a506c688e96",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eopch1 / 10,loss = 3.2012\n",
            "Eopch2 / 10,loss = 2.0728\n",
            "Eopch3 / 10,loss = 1.6986\n",
            "Eopch4 / 10,loss = 1.5136\n",
            "Eopch5 / 10,loss = 1.4014\n",
            "Eopch6 / 10,loss = 1.3213\n",
            "Eopch7 / 10,loss = 1.2608\n",
            "Eopch8 / 10,loss = 1.2138\n",
            "Eopch9 / 10,loss = 1.1757\n",
            "Eopch10 / 10,loss = 1.1411\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dataset = ParallelDataset(src_data,tgt_data,src_vocab,tgt_vocab)\n",
        "train_dataloader = DataLoader(dataset,batch_size = batch_size,shuffle = True)\n",
        "\n",
        "model = TransformerModel(src_vocab.vocab_size,tgt_vocab.vocab_size,d_model = 128,num_layers=3).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = Vocab.pad_idx)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for src_idx,tgt_idx in train_dataloader:\n",
        "        src_idx,tgt_idx = src_idx.to(device),tgt_idx.to(device)\n",
        "        # tgt在训练阶段，不用输入eos编码，因此需要切片\n",
        "        tgt_inputs_idx = tgt_idx[:,:-1]\n",
        "        tgt_target_idx = tgt_idx[:,1:]\n",
        "        # outputs。shape= （batch_size,tgt_seq_len,tgt_vocab_size)\n",
        "        outputs = model(src_idx,tgt_inputs_idx)\n",
        "        # 将输入展开：view将共用内存空间，只改变形状。reshape复制出新的张量\n",
        "        # view的张量，内存必须是连续的\n",
        "        # 这里tgt_target_idx是截取出来的张量，因此内存不连续，使用reshape\n",
        "        loss = criterion(outputs.view(-1,outputs.size(-1)),tgt_target_idx.reshape(-1))\n",
        "\n",
        "        # 梯度清零\n",
        "        optimizer.zero_grad()\n",
        "        # 反向传播\n",
        "        loss.backward()\n",
        "        # 更新参数\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_loss/=len(train_dataloader)\n",
        "\n",
        "    # 训练数据没有设置验证，由于机器翻译中，没有正确答案，一句话可以有多种翻译\n",
        "    # 因此模型评估时，无法再给出准确的评估值。\n",
        "    # 在模型训练时，可以给出target,给出准确答案，但在评估时，无法设置正确答案\n",
        "    print(f'Eopch{epoch+1} / {epochs},loss = {train_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "V3TcWBu6AAor",
        "outputId": "49e8a0a5-43d7-490d-a4ff-4962a1630cbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['stop', '!']\n",
            "tensor([[192,   1]])\n",
            "tensor([[  2, 791,   5,   1,  31,   3]], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'arrêtez de <unk> !'"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 模型翻译\n",
        "@torch.no_grad()\n",
        "def translate(model,src_sentence,max_length = 10,device = device):\n",
        "    model.eval()\n",
        "    src_tokens = list(src_sentence.split(' '))\n",
        "    src_idx = [src_vocab.word_index.get(token,Vocab.unk_idx) for token in src_tokens]\n",
        "    src_idx = torch.LongTensor(src_idx).unsqueeze(0)\n",
        "\n",
        "    tgt_idx = torch.LongTensor([[Vocab.sos_idx]])\n",
        "    print(src_tokens)\n",
        "    print(src_idx)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        src_idx,tgt_idx = src_idx.to(device),tgt_idx.to(device)\n",
        "        outputs = model(src_idx,tgt_idx)\n",
        "        next_token = outputs[0].argmax(dim=1)[-1:]\n",
        "        tgt_idx = torch.cat([tgt_idx,next_token.unsqueeze(0)],dim=1)\n",
        "        if next_token.item() == Vocab.eos_idx:\n",
        "            break\n",
        "    print(tgt_idx)\n",
        "    tgt_tokens = []\n",
        "    # 切片，用来去掉开始符号和结束符号\n",
        "    for idx in tgt_idx.squeeze().tolist()[1:-1]:\n",
        "        tgt_tokens.append(tgt_vocab.index_word[idx])\n",
        "    return ' '.join(tgt_tokens)\n",
        "\n",
        "translate(model,'stop !')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVCEmR9ParCm"
      },
      "outputs": [],
      "source": [
        "# 遇到问题：\n",
        "# 1、掩码设备问题：tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "#   在创建模型时候，给掩码藏家设备，与输入相同\n",
        "# 2、在翻译时，给输入增加设备到CUDA"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MLstudy",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
