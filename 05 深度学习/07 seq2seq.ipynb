{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器的最后一步输出，作为解码器的初始状态。\n",
    "# 解码器每一步预测输出，作为下一步输入。\n",
    "# 有两种停止方式，一是出现指定字符，二是固定长度。\n",
    "# 为加速训练收敛，增加了教师强制（teacher Forcing）机制。将真实值作为下一步输入，加速收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解压数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def extract_file(root_path,extract_path):\n",
    "    '''\n",
    "    解压文件\n",
    "    '''\n",
    "    if os.path.exists(extract_path):\n",
    "        os.makedirs(extract_path)\n",
    "    with zipfile.ZipFile(root_path,'r') as zip:\n",
    "        zip.extract(path = extract_path)\n",
    "\n",
    "root_path = r''\n",
    "extract_path = r''\n",
    "extract_file(root_path,extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词汇表类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 2, 'am': 2, 'a': 2, 'student': 1, 'teacher': 1}\n",
      "{'I': 2, 'am': 3, 'a': 4, 'student': 5, 'teacher': 6}\n",
      "{0: 'sos', 1: 'eos', 2: 'I', 3: 'am', 4: 'a', 5: 'student', 6: 'teacher'}\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# 定义开始字符与结束字符号的索引\n",
    "sos_token = 0\n",
    "eos_token = 1\n",
    "\n",
    "class Lang:\n",
    "    '''词汇表类，用于构建词汇表和管理词汇表\n",
    "    支持将单词映射为对应的索引，并记录每个单词的出现次数\n",
    "    '''\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.word2idx = {}\n",
    "        self.word2count = {}\n",
    "        self.idx2word = {0:'sos',1:'eos'}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self,word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.idx2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        \n",
    "lang = Lang('eng')\n",
    "lang.addSentence('I am a student')\n",
    "lang.addSentence('I am a teacher')\n",
    "\n",
    "print(lang.word2count)\n",
    "print(lang.word2idx)\n",
    "print(lang.idx2word)\n",
    "print(lang.n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据集，并创建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def UnicodeToAsceli(s):\n",
    "    '''\n",
    "    将字符内容正规化\n",
    "    '''\n",
    "    s = unicodedata.normalize('NFD',s)\n",
    "    return ''.join(c for c in s if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    '''\n",
    "    将字符内容正规化，小写，并去除特殊字符\n",
    "    '''\n",
    "    s = UnicodeToAsceli(s.lower().strip())\n",
    "    # \\1 表示引用第一个捕获组，将该组内容前面加一个空格\n",
    "    s = re.sub(r'([.?!])',r' \\1',s)\n",
    "    # 将英文吗，！？之外的符号替换为空格\n",
    "    s = re.sub(r'[^a-zA-Z!?]',r' ',s)\n",
    "    return s.strip()\n",
    "\n",
    "def readlangs(lang1,lang2,reverse = False):\n",
    "    '''从文件中读取平行数据集，并创建词汇表\n",
    "    parameter:\n",
    "    ------------\n",
    "    lang1:str\n",
    "        语言1名称（元语言）\n",
    "    lang2:ste\n",
    "        语言2名称（目标语言）\n",
    "    reverse:bool\n",
    "        是否颠倒平行数据\n",
    "    '''\n",
    "    pairs = []\n",
    "    with open(r'.\\data\\eng-fra.txt' , encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            pairs.append([normalizeString(s) for s in line.strip().split('\\t')])\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang,output_lang,pairs\n",
    "\n",
    "i,o ,pairs = readlangs('eng','fra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wait !', 'attends  !'],\n",
       " ['wait !', 'attendez  !'],\n",
       " ['i see', 'je comprends'],\n",
       " ['i try', 'j essaye'],\n",
       " ['i won !', 'j ai gagne  !']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置序列最大长度\n",
    "max_length = 10\n",
    "\n",
    "# prefixes 表示英语的前缀\n",
    "# 在正则处理中，将'都处理为空格，因此，需要将一些内容替换掉\n",
    "eng_prefixes = ('i am','i m',\n",
    "                'he is','he s',\n",
    "                'she is','she s',\n",
    "                'you are','you r',\n",
    "                'they are','they re',\n",
    "                'we are','we re')\n",
    "\n",
    "def filterPair(p):\n",
    "    con1 = len(p[0].split(' ')) < max_length\n",
    "    con2 = len(p[1].split(' ')) < max_length\n",
    "    con3 = p[1].startswith(eng_prefixes)\n",
    "\n",
    "    return con1 and con2 and con3\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取语句数： 135842\n",
      "过滤之后的数据长度： 12684\n",
      "fra 5192\n",
      "eng 3419\n",
      "['je me rejouis de vous voir danser', 'i m looking forward to seeing you dance']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def preparData(lang1,lang2,reverse =False):\n",
    "    # 读取词汇数据\n",
    "    input_lang,output_lang,pairs = readlangs(lang1,lang2,reverse)\n",
    "    print('读取语句数：',len(pairs))\n",
    "    # 过滤词汇数据\n",
    "    pairs = filterPairs(pairs)\n",
    "    print('过滤之后的数据长度：',len(pairs))\n",
    "\n",
    "    # 将过滤后的词汇添加到词汇表中\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    \n",
    "    print(input_lang.name,input_lang.n_words)\n",
    "    print(output_lang.name,output_lang.n_words)\n",
    "\n",
    "    return input_lang,output_lang,pairs\n",
    "\n",
    "input_lang,output_lang,pairs = preparData('eng','fra',True)\n",
    "print(random.choice(pairs))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as fc\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_size,dropout_p = 0.1):\n",
    "        '''\n",
    "        初始化方法\n",
    "        \n",
    "        parameter:\n",
    "        -------------\n",
    "        vocab_size:int\n",
    "            词嵌入的数量\n",
    "        embedding_dim:int\n",
    "            词嵌入的维度\n",
    "        hidden_size:int\n",
    "            RNN的数量\n",
    "        dropout_p:float\n",
    "            神经元丢弃比例\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim,hidden_size,batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        # input shape = (batch_size,seq_len)\n",
    "        # embedding shape = (batch_size,seq_len,embedding_dim)\n",
    "        embeding_x = self.embedding(x)\n",
    "        # output shape = (batch_size,seq_len,D*hidden_size)\n",
    "        # hidden shape = (D*num_layer,batch_size,hidden_size)\n",
    "        output,hidden = self.gru(embeding_x)\n",
    "        \n",
    "        return output,hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embedding_dim,hidden_size,vocab_size):\n",
    "        '''\n",
    "        初始化方法\n",
    "        \n",
    "        parameter\n",
    "        --------------\n",
    "        embedding_dim:int\n",
    "            解码器词嵌入的维度\n",
    "        hidden_size:int\n",
    "            RNN的维度\n",
    "        output_size:int\n",
    "            就是词嵌入的数量\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim,hidden_size,batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size,vocab_size)\n",
    "\n",
    "    def forward(self,encoder_outputs,encoder_hidden,target_tensor = None,):\n",
    "        '''\n",
    "        parameter\n",
    "        ----------------\n",
    "        encoder_output:tensor ,shape = (batch_size,seq_len,hidden_size)\n",
    "            编码器每个时间步的输出(用于注意力机制)\n",
    "        encoder_hidden:tensor,shape = (d*num_layer,batch_size,hidden_size)\n",
    "            编码器的隐藏状态输出,作为解码器的初始隐藏状态\n",
    "        target_tensor: tensor,shape=(batch_size,target_seq_len)\n",
    "            目标值，为未经过embeddding的输入数据，用于训练时开启教师强制\n",
    "        \n",
    "        return\n",
    "        ----------------------\n",
    "        decoder_output:tensor,shape = (batch_size,tager_seq_len,vocab_size) \n",
    "            解码器所有时间步的输出层的输出！ 注意是输出层，不是RNN层的输出\n",
    "        decoder_hidden:tensor ,shape = (d*num_layer,batch_size,tager_seq_len)\n",
    "        '''\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        # 初始化，解码器的输入，使用SOS——token(开始字符)\n",
    "        decoder_input = torch.empty(batch_size,1,dtype = torch.long,device=device).fill_(sos_token) # 自己创建的张量，需要指定类型、设备\n",
    "        # 使用编码器的隐藏状态，初始化解码器的初始隐藏状态\n",
    "        decoder_hidden = encoder_hidden\n",
    "        # 用来储存每个时间步的输出\n",
    "        decoder_outputs = []\n",
    "        for i in range(max_length):\n",
    "            decoder_output ,decoder_hidden = self.forward_step(decoder_input,decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            if target_tensor is not None:\n",
    "                # 如果使用教师强制，则使用教师强制的输入\n",
    "                decoder_input = target_tensor[:,i].unsequenze(1)\n",
    "            else:\n",
    "                # 如果不使用教师强制，使用当前输出概率最高的词，作为输出\n",
    "                out = decoder_output.argmax(dim = -1)\n",
    "                # 每一个输入，不应该与上一个输出相连，否则会有梯度在之间传播，历史就会影响未来\n",
    "                decoder_input = out.detach() # 使用ditach,断开梯度传播\n",
    "\n",
    "\n",
    "    def forward_step(self,input,hidden):\n",
    "        \n",
    "        pass\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
