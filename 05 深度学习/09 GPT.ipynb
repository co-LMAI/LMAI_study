{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT训练\n",
    "    - 无监督训练：以自回归的方式，基于transformer解码器原理，采用掩码自注意力机制,基于字节对编码算法（byte-pair Encoding）,将文本分割为小单元    \n",
    "    - 有监督微调：根据下游任务(如文本分类、问答、摘要)调整模型参数，使其具有特定领域能力,联合优化任务损失和训练损失\n",
    "# 解码策略\n",
    "    - 贪心算法：每次预测都取概率最大的.方法比较简单，但结果全是相同的。只考虑当前最优解，忽视全局最优解.  \n",
    "        --top K:从概率超过K的中，随机选择一个.,top P方法优化  \n",
    "        --top P:采用累计分布概率，设定P值,选择累积概率不超过P的序列\n",
    "        --温度调节:每一个词生产一个logits,使用logits/tem,用来控制logits的分布，tem越高，则越有创意，tem越低则越保守\n",
    "    - 束搜索：生成过程，\n",
    "        --有一个beam size=K，每次都在选择K长度的序列，计算联合概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as fc\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self,vocab_size = 10000,embed_dim = 128,num_layer = 3,max_len = 512):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.pos_embed = nn.Embedding(max_len,embed_dim)\n",
    "        # 使用编码器层代替GPT解码器\n",
    "        # pass\n",
    "\n",
    "        # 最小改动解码器，实现GPT\n",
    "        self.layers = nn.ModuleList([\n",
    "                                    nn.TransformerDecoderLayer(\n",
    "                                        d_model=embed_dim,\n",
    "                                        nhead=4,\n",
    "                                        dim_feedforward=embed_dim*4,\n",
    "                                        dropout = 0.1,\n",
    "                                        batch_first = True,                        \n",
    "                                    ) for _ in range(num_layer)\n",
    "                                    ])\n",
    "        # 为什么使用ModuleList？使用普通列表，无法将所有参数绑定到类中，使用ModuleList可以将所有可训练parameter()收集\n",
    "        # 语言模型的输出层\n",
    "        self.lm_out  = nn.Linear(embed_dim,vocab_size)\n",
    "        # 分类层：用于模型微调时添加\n",
    "        self.classifier = None\n",
    "\n",
    "    def forward(self,input_ids):\n",
    "        # input_ids.shape = (batch_size,seq_len)\n",
    "        seq_len = input_ids.size(1)\n",
    "        pos = torch.arange(seq_len,device=input_ids.device)\n",
    "        # self.token_embed(input_ids).shape = (seq_len,embed_dim)\n",
    "        x  = self.token_embed(input_ids)+self.pos_embed(pos)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(seq_len,device=input_ids.device)\n",
    "        for layer in self.layers:\n",
    "            x = layer(tgt=x,memory = x,tgt_mask = mask)\n",
    "        # 语言模型输出\n",
    "        logits = self.lm_out(x)\n",
    "        # 分类任务输出\n",
    "        if self.classifier is not None:\n",
    "            # 取序列中的首token,作为分类\n",
    "            cls_logits = self.classifier(x[:,0,:])\n",
    "            return logits,cls_logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语言模型的形状 torch.Size([2, 4, 100])\n",
      "分类logits torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    vocab_size = 100\n",
    "    embed_dim = 64\n",
    "    n_layer = 6\n",
    "    max_len = 16\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "\n",
    "    model = MiniGPT(vocab_size,embed_dim,n_layer,max_len)\n",
    "    input_ids = torch.randint(0,vocab_size,(batch_size,seq_len))\n",
    "    logits = model(input_ids)\n",
    "    print('语言模型的形状',logits.shape)\n",
    "    model.classifier = nn.Linear(embed_dim,3)\n",
    "    logits,cls_logits = model(input_ids)\n",
    "    print('分类logits',cls_logits.shape)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    '''文本数据集类，用于预训练模型'''\n",
    "    def __init__(self,file_path,tokenizer,block_size = 512,stride = 384):\n",
    "        '''\n",
    "        parameter\n",
    "        ------------\n",
    "        tokenizer:AotoTonkenizer\n",
    "            分词器\n",
    "        block_size:int\n",
    "            分块大小\n",
    "        stride:int\n",
    "            滑动的步长\n",
    "        '''\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.stride = stride\n",
    "\n",
    "        with open(file_path,mode = 'r',encoding='utf-8') as f:\n",
    "            text = self.clear_text(f.read())\n",
    "        self.tokens = self.tokenizer.encode(text,add_special_tokens = True)\n",
    "        # 若数据量很大，使用动态截取的方式，而不是事先全部计算\n",
    "\n",
    "    def clear_text(self,text):\n",
    "        # 段落间的分隔符\n",
    "        token = self.tokenizer.sep_token + self.tokenizer.cls_token\n",
    "        text = text.strip()\n",
    "        text = re.sub('\\n',token,text)\n",
    "        text = re.sub(r'\\s+','',text)\n",
    "        return text\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 计算总样本数:（总长度-块长度）/步长+1\n",
    "        return (len(self.tokens)-self.block_size)//self.stride+1\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        start = idx * self.stride\n",
    "        end = start + self.block_size\n",
    "        # 动态截取token块。\n",
    "        chunk = self.tokens[start:end]\n",
    "        x = torch.tensor(chunk[:-1],dtype = torch.long)\n",
    "        y = torch.tensor(chunk[1:],dtype = torch.long)\n",
    "        return x,y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('uer/gpt2-chinese-cluecorpussmall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:1 train loss:9.6956: 100%|██████████| 2/2 [00:00<00:00,  4.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.903632164001465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:2 train loss:9.0865: 100%|██████████| 2/2 [00:00<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.212864398956299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:3 train loss:8.6985: 100%|██████████| 2/2 [00:00<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.788248062133789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:4 train loss:8.2966: 100%|██████████| 2/2 [00:00<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.402206420898438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:5 train loss:7.8265: 100%|██████████| 2/2 [00:00<00:00, 10.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.955410957336426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:6 train loss:7.3768: 100%|██████████| 2/2 [00:00<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4914422035217285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:7 train loss:6.9836: 100%|██████████| 2/2 [00:00<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.076613426208496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:8 train loss:6.5668: 100%|██████████| 2/2 [00:00<00:00, 11.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.6882734298706055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:9 train loss:6.2824: 100%|██████████| 2/2 [00:00<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.343282699584961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:10 train loss:5.9418: 100%|██████████| 2/2 [00:00<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.015002250671387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:11 train loss:5.6256: 100%|██████████| 2/2 [00:00<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.722519874572754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:12 train loss:5.4582: 100%|██████████| 2/2 [00:00<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.490045070648193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:13 train loss:5.2236: 100%|██████████| 2/2 [00:00<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.265669345855713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:14 train loss:4.9554: 100%|██████████| 2/2 [00:00<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.051986217498779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:15 train loss:4.8225: 100%|██████████| 2/2 [00:00<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.882689714431763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:16 train loss:4.7110: 100%|██████████| 2/2 [00:00<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.727940797805786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:17 train loss:4.5061: 100%|██████████| 2/2 [00:00<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.564103126525879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:18 train loss:4.4196: 100%|██████████| 2/2 [00:00<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4329445362091064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:19 train loss:4.2171: 100%|██████████| 2/2 [00:00<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.277642250061035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:20 train loss:4.0521: 100%|██████████| 2/2 [00:00<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.134768724441528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:21 train loss:4.0568: 100%|██████████| 2/2 [00:00<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.026669859886169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:22 train loss:3.8333: 100%|██████████| 2/2 [00:00<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8760769367218018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:23 train loss:3.7208: 100%|██████████| 2/2 [00:00<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7517181634902954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:24 train loss:3.6317: 100%|██████████| 2/2 [00:00<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6328240633010864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:25 train loss:3.4529: 100%|██████████| 2/2 [00:00<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4954456090927124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:26 train loss:3.3564: 100%|██████████| 2/2 [00:00<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.376646876335144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:27 train loss:3.2548: 100%|██████████| 2/2 [00:00<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2577600479125977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:28 train loss:3.1069: 100%|██████████| 2/2 [00:00<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1291568279266357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:29 train loss:2.9360: 100%|██████████| 2/2 [00:00<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9945675134658813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:30 train loss:2.9221: 100%|██████████| 2/2 [00:00<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.89854896068573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:31 train loss:2.7803: 100%|██████████| 2/2 [00:00<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.776131749153137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:32 train loss:2.6845: 100%|██████████| 2/2 [00:00<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6639904975891113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:33 train loss:2.5179: 100%|██████████| 2/2 [00:00<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.53777813911438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:34 train loss:2.4517: 100%|██████████| 2/2 [00:00<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4370702505111694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:35 train loss:2.3002: 100%|██████████| 2/2 [00:00<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.318273663520813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:36 train loss:2.1749: 100%|██████████| 2/2 [00:00<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2085143327713013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:37 train loss:2.1177: 100%|██████████| 2/2 [00:00<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.112419843673706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:38 train loss:2.0376: 100%|██████████| 2/2 [00:00<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0101239681243896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:39 train loss:1.9322: 100%|██████████| 2/2 [00:00<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9117707014083862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:40 train loss:1.7854: 100%|██████████| 2/2 [00:00<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.805662751197815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:41 train loss:1.6884: 100%|██████████| 2/2 [00:00<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.708232343196869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:42 train loss:1.6106: 100%|██████████| 2/2 [00:00<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6194342374801636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:43 train loss:1.4606: 100%|██████████| 2/2 [00:00<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5109540820121765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:44 train loss:1.4213: 100%|██████████| 2/2 [00:00<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4376680850982666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:45 train loss:1.2803: 100%|██████████| 2/2 [00:00<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3484402298927307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:46 train loss:1.2223: 100%|██████████| 2/2 [00:00<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2730331420898438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:47 train loss:1.1346: 100%|██████████| 2/2 [00:00<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.193272054195404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:48 train loss:1.0892: 100%|██████████| 2/2 [00:00<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.118008553981781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:49 train loss:0.9934: 100%|██████████| 2/2 [00:00<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0414519309997559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:50 train loss:0.9763: 100%|██████████| 2/2 [00:00<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9846748113632202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "block_size = 64\n",
    "stride = 64\n",
    "batch_size = 64\n",
    "embed_dim = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 50\n",
    "def pretrain():\n",
    "    dataset = TextDataset(\n",
    "        file_path=r'E:\\LMAI_study\\05 深度学习\\data\\天龙八部分类.txt',\n",
    "        tokenizer=tokenizer,\n",
    "        block_size=block_size,\n",
    "        stride=stride\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(dataset,batch_size=batch_size,shuffle = True)\n",
    "\n",
    "    model = MiniGPT(vocab_size=tokenizer.vocab_size,embed_dim=embed_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        bar = tqdm(train_loader)\n",
    "        for input,target in bar:\n",
    "            input ,target = input.to(device) ,target.to(device)\n",
    "            logits =  model(input)\n",
    "            # 对不连续的变量改变形状，不能直接用view,可以用reshape,或调用contiguous方法\n",
    "            loss = criterion(logits.view(-1,tokenizer.vocab_size),target.contiguous().view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "            # 训练一个批次后，显示当前批次的损失\n",
    "            bar.set_description(f'epoch:{epoch+1} train loss:{loss.item():.4f}')\n",
    "        train_loss /= len(train_loader)\n",
    "        print(train_loss)\n",
    "    torch.save(model.state_dict(),'model.pth')\n",
    "\n",
    "pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
