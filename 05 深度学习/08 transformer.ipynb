{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer\n",
    "\n",
    "    - 位置编码：使得输入序列并行训练\n",
    "    - 多头自注意力机制：\n",
    "    - 多头注意力机制：\n",
    "    - add & Nom : 残差连接、层归一化\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正弦、余弦函数编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置编码张量形状 torch.Size([5, 2])\n",
      "tensor([[[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# 正弦、余弦函数编码\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncodeing(nn.Module):\n",
    "    def __init__(self,d_model,max_length = 1000):\n",
    "        '''初始化方法\n",
    "        \n",
    "        parameter\n",
    "        --------------\n",
    "        d_model ：int\n",
    "            嵌入向量维度\n",
    "        max_length ：int\n",
    "            最大序列长度    \n",
    "        '''\n",
    "        super().__init__()\n",
    "        # 创建位置编码矩阵\n",
    "        pe = torch.zeros(max_length,d_model)\n",
    "        # 创建一个一维张量，其元素为从0到max_length-1，便是序列中的各个位置\n",
    "        # 将形状转（max_length,)换为（max_length,1)，便于后续计算\n",
    "        position = torch.arange(0,max_length,dtype=torch.float).unsqueeze(1)\n",
    "        # exp(log(a)*b) = a^b\n",
    "\n",
    "        div_trem = torch.exp(torch.arange(0,d_model,2) * (-np.log(10000.0)/d_model))\n",
    "        # d_model必须为偶数，保证奇数长度与偶数长度相同\n",
    "        # Position*div_trem.shape = (max_length,d_model/2)\n",
    "        pe[:,0::2] = torch.sin(position * div_trem)\n",
    "        pe[:,1::2] = torch.cos(position * div_trem)\n",
    "        # 将pe注册为模型的缓冲区\n",
    "        # 缓冲区时pytorch中的一种特殊属性，其不会被计算图追踪，不会更新梯度\n",
    "        # 但是，成为缓冲区后，会成为state_dict的一部分，会随着模型一起保存和加载\n",
    "        # 当注册缓冲区后，变量就会绑定当前对象，成为当前对象属性\n",
    "        # 注册属性与绑定属性的区别:\n",
    "            # 1、缓冲区会随着模型一起保存和加载，但是绑定属性无此功能\n",
    "            # 2、缓冲区与模型参数一样，会随着模型一起迁移，但绑定属性无此功能\n",
    "        self.register_buffer('pe',pe)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # x.shape = (batch_size,seq_length,d_model)\n",
    "        # 将词嵌入向量与位置张量相加\n",
    "        x + self.pe[:x.size(1)]\n",
    "        return x\n",
    "    \n",
    "def test():\n",
    "    d_model = 2\n",
    "    max_length = 5\n",
    "    batch_szie = 2\n",
    "    pos = PositionalEncodeing(d_model,max_length)\n",
    "    print('位置编码张量形状',pos.pe.shape)\n",
    "\n",
    "    x = torch.zeros(batch_szie,max_length,d_model)\n",
    "    x_with_pos = pos(x)\n",
    "    print(x_with_pos)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可训练位置编码  \n",
    "优势：  \n",
    "    - 灵活性，更具任务自适应调整  \n",
    "    - 模型性能，可训练编码可以带给模型更好的性能\n",
    "\n",
    "缺点：  \n",
    "    - 外推能力，由于编码可训练，无法更好的泛化到训练时为见过的序列长度  \n",
    "    - 计算成本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置编码张量形状 torch.Size([5, 2])\n",
      "tensor([[[ 0.6606, -1.6223],\n",
      "         [ 0.4468,  0.9839],\n",
      "         [ 0.3566, -0.3273],\n",
      "         [ 1.0180, -0.0404],\n",
      "         [ 0.8518,  0.1466]],\n",
      "\n",
      "        [[ 0.6606, -1.6223],\n",
      "         [ 0.4468,  0.9839],\n",
      "         [ 0.3566, -0.3273],\n",
      "         [ 1.0180, -0.0404],\n",
      "         [ 0.8518,  0.1466]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncodeing2(nn.Module):\n",
    "    '''可训练位置编码\n",
    "    通过可训练的嵌入层，实现位置编码\n",
    "    '''\n",
    "\n",
    "    def __init__(self,d_model,max_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        # 创建可学习的位置嵌入\n",
    "        self.position_embedding = nn.Embedding(max_length,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        seq_len = x.size(1)\n",
    "        # 生成位置索引\n",
    "        pos_indices = torch.arange(seq_len,device =x.device,dtype=torch.long).unsqueeze(0)\n",
    "        # 从嵌入层中获取位置编码\n",
    "        pos_embedding = self.position_embedding(pos_indices)\n",
    "        return x+pos_embedding\n",
    "    \n",
    "def test():\n",
    "    d_model = 2\n",
    "    max_length = 5\n",
    "    batch_szie = 2\n",
    "    pos = PositionalEncodeing2(d_model,max_length)\n",
    "    print('位置编码张量形状',pos.position_embedding.weight.shape)\n",
    "\n",
    "    x = torch.zeros(batch_szie,max_length,d_model)\n",
    "    x_with_pos = pos(x)\n",
    "    print(x_with_pos)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力机制\n",
    "q,k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自注意力机制\n",
    "当q,k,v都是同一个张量，就是自注意力机制    \n",
    "为什么需要自注意力，因为输入的词，经过词嵌入、位置编码，但依然没有彼此之间的关系，自注意力机制，就是，词与词之间的关系  \n",
    "self_attention.shape = (seq_length,seq_length)  \n",
    "例：  \n",
    "seq = [x1,x3,x3,x4].shape = (n)  \n",
    "embedding_seq.shape = (n,d_model)  \n",
    "self_attention_seq = [[w1*x1+w2*x2+w3*x3+w4*x4],\n",
    "                      [w1*x1+w2*x2+w3*x3+w4*x4],\n",
    "                      [w1*x1+w2*x2+w3*x3+w4*x4].\n",
    "                      [w1*x1+w2*x2+w3*x3+w4*x4]]  \n",
    "self_attention_seq.shape = (n,d_model)                         \n",
    "序列x.shape = (n,d_model)  \n",
    "q = x*wq wq.shape = (d_model,dq)  \n",
    "k = x*wk wk.shape = (d_model,dk)  \n",
    "v = x*wv wv.shape = (d_model,dv)\n",
    "\n",
    "scores = q*k.T/sqrt(dk)  \n",
    "weigths = softmax(scores)  \n",
    "context = weigths*v\n",
    "\n",
    "\n",
    "## 多头注意力机制\n",
    "将注意力矩阵中的QKV，切分成多份  \n",
    "q_h = x*wq wq.shape = (d_model/h,dq)  \n",
    "k_h = x*wk wk.shape = (d_model/h,dk)  \n",
    "v_h = x*wv wv.shape = (d_model/h,dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_head,p = 0.1):\n",
    "        super().__init__()\n",
    "        if d_model % num_head != 0:\n",
    "            raise ValueError(f'd_model({d_model})需要能被num_head({num_head})整除')\n",
    "        self.d_model = d_model\n",
    "        self.num_head =num_head\n",
    "        self.head_dim = d_model//num_head\n",
    "        self.q_proj = nn.Linear(d_model,d_model)\n",
    "        self.k_proj = nn.Linear(d_model,d_model)\n",
    "        self.v_proj = nn.Linear(d_model,d_model)\n",
    "        self.out_proj = nn.Linear(d_model,d_model)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self,query,keys,values,atten_mask = None,key_padding_mask = None):\n",
    "        '''parameters\n",
    "        -----------------\n",
    "        query:torh.tensor shape = (batch_size,traget_seq_len,d_model)\n",
    "            查询张量，在编码器中，target就是srcoe\n",
    "        keys:torh.tensor shape = (batch_size,src_seq_len,d_model)\n",
    "        values:torh.tensor shape = (batch_size,src_seq_len,d_model)\n",
    "\n",
    "        return\n",
    "        ---------------------\n",
    "        output : torch.tensor shape = (batch_szie,tgt_seq_length,d_model)\n",
    "        atten_weigths : torch.tensor shape = (batch_szie,num_head,tgt_seq_length,src_seq_length)\n",
    "        '''\n",
    "        batch_szie = query.size(0)\n",
    "        # 线性变换\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(keys)\n",
    "        v = self.v_proj(values)\n",
    "\n",
    "        # 将qkv拆分为多个头\n",
    "        # q.shape = (batch_szie,num_head,tgt_seq_length,head_dim)\n",
    "        q = q.view(batch_szie,-1,self.num_head,self.head_dim).transpose(1,2)\n",
    "        k = k.view(batch_szie,-1,self.num_head,self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_szie,-1,self.num_head,self.head_dim).transpose(1,2)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        scores = torch.matmul(q,k.transpose(-2,-1))/self.head_dim**0.5\n",
    "        atten_weigths = torch.softmax(scores,dim=-1)\n",
    "        atten_weigths = self.dropout(atten_weigths)\n",
    "\n",
    "        # 计算注意力输出\n",
    "        atten_output = torch.matmul(atten_weigths,v)\n",
    "\n",
    "        # 合并多头\n",
    "        atten_output = atten_output.transpose(1,2).contiguous().view(batch_szie,-1,self.d_model)\n",
    "        # 线性变换输出\n",
    "        output = self.out_proj(atten_output)\n",
    "        return output,atten_weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状 torch.Size([2, 3, 10])\n",
      "注意力权重形状 torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    d_model = 10\n",
    "    batch_szie = 2\n",
    "    src_seq_length = 4\n",
    "    tgt_seq_length = 3\n",
    "    num_head = 2\n",
    "\n",
    "\n",
    "    q = torch.randn(batch_szie,tgt_seq_length,d_model)\n",
    "    k = torch.randn(batch_szie,src_seq_length,d_model)\n",
    "    v = torch.randn(batch_szie,src_seq_length,d_model)\n",
    "\n",
    "    atten = MultiHeadAttention(d_model,num_head)\n",
    "    output,atten_weigths = atten(q,k,v)\n",
    "    print('输出形状',output.shape)\n",
    "    print('注意力权重形状',atten_weigths.shape)\n",
    "test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
