{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer\n",
    "\n",
    "    - 位置编码：使得输入序列并行训练\n",
    "    - 多头自注意力机制：\n",
    "    - 多头注意力机制：\n",
    "    - add & Nom : 残差连接、层归一化\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正弦、余弦函数编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置编码张量形状 torch.Size([5, 2])\n",
      "tensor([[[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# 正弦、余弦函数编码\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncodeing(nn.Module):\n",
    "    def __init__(self,d_model,max_length = 1000):\n",
    "        '''初始化方法\n",
    "        \n",
    "        parameter\n",
    "        --------------\n",
    "        d_model ：int\n",
    "            嵌入向量维度\n",
    "        max_length ：int\n",
    "            最大序列长度    \n",
    "        '''\n",
    "        super().__init__()\n",
    "        # 创建位置编码矩阵\n",
    "        pe = torch.zeros(max_length,d_model)\n",
    "        # 创建一个一维张量，其元素为从0到max_length-1，便是序列中的各个位置\n",
    "        # 将形状转（max_length,)换为（max_length,1)，便于后续计算\n",
    "        position = torch.arange(0,max_length,dtype=torch.float).unsqueeze(1)\n",
    "        # exp(log(a)*b) = a^b\n",
    "\n",
    "        div_trem = torch.exp(torch.arange(0,d_model,2) * (-np.log(10000.0)/d_model))\n",
    "        # d_model必须为偶数，保证奇数长度与偶数长度相同\n",
    "        # Position*div_trem.shape = (max_length,d_model/2)\n",
    "        pe[:,0::2] = torch.sin(position * div_trem)\n",
    "        pe[:,1::2] = torch.cos(position * div_trem)\n",
    "        # 将pe注册为模型的缓冲区\n",
    "        # 缓冲区时pytorch中的一种特殊属性，其不会被计算图追踪，不会更新梯度\n",
    "        # 但是，成为缓冲区后，会成为state_dict的一部分，会随着模型一起保存和加载\n",
    "        # 当注册缓冲区后，变量就会绑定当前对象，成为当前对象属性\n",
    "        # 注册属性与绑定属性的区别:\n",
    "            # 1、缓冲区会随着模型一起保存和加载，但是绑定属性无此功能\n",
    "            # 2、缓冲区与模型参数一样，会随着模型一起迁移，但绑定属性无此功能\n",
    "        self.register_buffer('pe',pe)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # x.shape = (batch_size,seq_length,d_model)\n",
    "        # 将词嵌入向量与位置张量相加\n",
    "        x + self.pe[:x.size(1)]\n",
    "        return x\n",
    "    \n",
    "def test():\n",
    "    d_model = 2\n",
    "    max_length = 5\n",
    "    batch_szie = 2\n",
    "    pos = PositionalEncodeing(d_model,max_length)\n",
    "    print('位置编码张量形状',pos.pe.shape)\n",
    "\n",
    "    x = torch.zeros(batch_szie,max_length,d_model)\n",
    "    x_with_pos = pos(x)\n",
    "    print(x_with_pos)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可训练位置编码  \n",
    "优势：  \n",
    "    - 灵活性，更具任务自适应调整  \n",
    "    - 模型性能，可训练编码可以带给模型更好的性能\n",
    "\n",
    "缺点：  \n",
    "    - 外推能力，由于编码可训练，无法更好的泛化到训练时为见过的序列长度  \n",
    "    - 计算成本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置编码张量形状 torch.Size([5, 2])\n",
      "tensor([[[-1.3116, -0.7478],\n",
      "         [-0.5468,  0.0461],\n",
      "         [-0.2335,  1.6065],\n",
      "         [-0.7880,  0.4872],\n",
      "         [ 0.4613,  0.4113]],\n",
      "\n",
      "        [[-1.3116, -0.7478],\n",
      "         [-0.5468,  0.0461],\n",
      "         [-0.2335,  1.6065],\n",
      "         [-0.7880,  0.4872],\n",
      "         [ 0.4613,  0.4113]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncodeing2(nn.Module):\n",
    "    '''可训练位置编码\n",
    "    通过可训练的嵌入层，实现位置编码\n",
    "    '''\n",
    "\n",
    "    def __init__(self,d_model,max_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        # 创建可学习的位置嵌入\n",
    "        self.position_embedding = nn.Embedding(max_length,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        seq_len = x.size(1)\n",
    "        # 生成位置索引\n",
    "        pos_indices = torch.arange(seq_len,device =x.device,dtype=torch.long).unsqueeze(0)\n",
    "        # 从嵌入层中获取位置编码\n",
    "        pos_embedding = self.position_embedding(pos_indices)\n",
    "        return x+pos_embedding\n",
    "    \n",
    "def test():\n",
    "    d_model = 2\n",
    "    max_length = 5\n",
    "    batch_szie = 2\n",
    "    pos = PositionalEncodeing2(d_model,max_length)\n",
    "    print('位置编码张量形状',pos.position_embedding.weight.shape)\n",
    "\n",
    "    x = torch.zeros(batch_szie,max_length,d_model)\n",
    "    x_with_pos = pos(x)\n",
    "    print(x_with_pos)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力机制\n",
    "q,k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自注意力机制\n",
    "当q,k,v都是同一个张量，就是自注意力机制    \n",
    "为什么需要自注意力，因为输入的词，经过词嵌入、位置编码，但依然没有彼此之间的关系，自注意力机制，就是，词与词之间的关系  \n",
    "self_attention.shape = (seq_length,seq_length)  \n",
    "例：  \n",
    "seq = [x1,x3,x3,x4].shape = (n)  \n",
    "embedding_seq.shape = (n,d_model)  \n",
    "self_attention_seq = [[w1*x1+w2*x2+w3*x3+w4*x4],\n",
    "                      [w1*x1+w2*x2+w3*x3+w4*x4],\n",
    "                      [w1*x1+w2*x2+w3*x3+w4*x4].\n",
    "                      [w1*x1+w2*x2+w3*x3+w4*x4]]  \n",
    "self_attention_seq.shape = (n,d_model)                         \n",
    "序列x.shape = (n,d_model)  \n",
    "q = x*wq wq.shape = (d_model,dq)  \n",
    "k = x*wk wk.shape = (d_model,dk)  \n",
    "v = x*wv wv.shape = (d_model,dv)\n",
    "\n",
    "scores = q*k.T/sqrt(dk)  \n",
    "weigths = softmax(scores)  \n",
    "context = weigths*v\n",
    "\n",
    "\n",
    "## 多头自注意力机制\n",
    "将注意力矩阵中的QKV，切分成多份  \n",
    "q_h = x*wq wq.shape = (d_model/h,dq)  \n",
    "k_h = x*wk wk.shape = (d_model/h,dk)  \n",
    "v_h = x*wv wv.shape = (d_model/h,dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_head,p = 0.1):\n",
    "        super().__init__()\n",
    "        if d_model % num_head != 0:\n",
    "            raise ValueError(f'd_model({d_model})需要能被num_head({num_head})整除')\n",
    "        self.d_model = d_model\n",
    "        self.num_head =num_head\n",
    "        self.head_dim = d_model//num_head\n",
    "        self.q_proj = nn.Linear(d_model,d_model)\n",
    "        self.k_proj = nn.Linear(d_model,d_model)\n",
    "        self.v_proj = nn.Linear(d_model,d_model)\n",
    "        self.out_proj = nn.Linear(d_model,d_model)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self,query,keys,values,atten_mask = None,key_padding_mask = None):\n",
    "        '''parameters\n",
    "        -----------------\n",
    "        query:torh.tensor shape = (batch_size,traget_seq_len,d_model)\n",
    "            查询张量，在编码器中，target就是srcoe\n",
    "        keys:torh.tensor shape = (batch_size,src_seq_len,d_model)\n",
    "        values:torh.tensor shape = (batch_size,src_seq_len,d_model)\n",
    "\n",
    "        return\n",
    "        ---------------------\n",
    "        output : torch.tensor shape = (batch_szie,tgt_seq_length,d_model)\n",
    "        atten_weigths : torch.tensor shape = (batch_szie,num_head,tgt_seq_length,src_seq_length)\n",
    "        '''\n",
    "        batch_szie = query.size(0)\n",
    "        # 线性变换\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(keys)\n",
    "        v = self.v_proj(values)\n",
    "\n",
    "        # 将qkv拆分为多个头\n",
    "        # q.shape = (batch_szie,num_head,tgt_seq_length,head_dim)\n",
    "        q = q.view(batch_szie,-1,self.num_head,self.head_dim).transpose(1,2)\n",
    "        k = k.view(batch_szie,-1,self.num_head,self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_szie,-1,self.num_head,self.head_dim).transpose(1,2)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        scores = torch.matmul(q,k.transpose(-2,-1))/self.head_dim**0.5\n",
    "        atten_weigths = torch.softmax(scores,dim=-1)\n",
    "        atten_weigths = self.dropout(atten_weigths)\n",
    "\n",
    "        # 计算注意力输出\n",
    "        atten_output = torch.matmul(atten_weigths,v)\n",
    "\n",
    "        # 合并多头\n",
    "        atten_output = atten_output.transpose(1,2).contiguous().view(batch_szie,-1,self.d_model)\n",
    "        # 线性变换输出\n",
    "        output = self.out_proj(atten_output)\n",
    "        return output,atten_weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状 torch.Size([2, 3, 10])\n",
      "注意力权重形状 torch.Size([2, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    d_model = 10\n",
    "    batch_szie = 2\n",
    "    src_seq_length = 4\n",
    "    tgt_seq_length = 3\n",
    "    num_head = 2\n",
    "\n",
    "\n",
    "    q = torch.randn(batch_szie,tgt_seq_length,d_model)\n",
    "    k = torch.randn(batch_szie,src_seq_length,d_model)\n",
    "    v = torch.randn(batch_szie,src_seq_length,d_model)\n",
    "\n",
    "    atten = MultiHeadAttention(d_model,num_head)\n",
    "    output,atten_weigths = atten(q,k,v)\n",
    "    print('输出形状',output.shape)\n",
    "    print('注意力权重形状',atten_weigths.shape)\n",
    "test()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 掩码多头自注意力机制\n",
    "\n",
    "    解码器中，当输入第一个开始字符后，无法全部计算注意力，因此，需要设置掩码。  \n",
    "    在计算得到scores后，加一个M矩阵（上三角矩阵，值为-∞）  \n",
    "    然后进行，softmax计算\n",
    "\n",
    "      作用：\n",
    "        1、防止信息泄露\n",
    "        2、捕捉序列内部依赖关系\n",
    "        3、      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_head,p = 0.1):\n",
    "        super().__init__()\n",
    "        if d_model % num_head != 0:\n",
    "            raise ValueError(f'd_model({d_model})需要能被num_head({num_head})整除')\n",
    "        self.d_model = d_model\n",
    "        self.num_head =num_head\n",
    "        self.head_dim = d_model//num_head\n",
    "        self.q_proj = nn.Linear(d_model,d_model)\n",
    "        self.k_proj = nn.Linear(d_model,d_model)\n",
    "        self.v_proj = nn.Linear(d_model,d_model)\n",
    "        self.out_proj = nn.Linear(d_model,d_model)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self,query,keys,values,atten_mask = None,key_padding_mask = None):\n",
    "        '''parameters\n",
    "        -----------------\n",
    "        query:torh.tensor shape = (batch_size,traget_seq_len,d_model)\n",
    "            查询张量，在编码器中，target就是srcoe\n",
    "        keys:torh.tensor shape = (batch_size,src_seq_len,d_model)\n",
    "\n",
    "        values:torh.tensor shape = (batch_size,src_seq_len,d_model)\n",
    "\n",
    "        atten_mask : torch.tensor shape = (tgt_seq_len,src_seq_len)\n",
    "            注意力掩码张量\n",
    "        key_padding_mask : torch.tensor shape = (batch_size,src_seq_len) \n",
    "\n",
    "\n",
    "        return\n",
    "        ---------------------\n",
    "        output : torch.tensor shape = (batch_szie,tgt_seq_length,d_model)\n",
    "        atten_weigths : torch.tensor shape = (batch_szie,num_head,tgt_seq_length,src_seq_length)\n",
    "        '''\n",
    "        batch_szie = query.size(0)\n",
    "        # 线性变换\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(keys)\n",
    "        v = self.v_proj(values)\n",
    "\n",
    "        # 将qkv拆分为多个头\n",
    "        # q.shape = (batch_szie,num_head,tgt_seq_length,head_dim)\n",
    "        q = q.view(batch_szie,-1,self.num_head,self.head_dim).transpose(1,2)\n",
    "        k = k.view(batch_szie,-1,self.num_head,self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_szie,-1,self.num_head,self.head_dim).transpose(1,2)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        scores = torch.matmul(q,k.transpose(-2,-1))/self.head_dim**0.5\n",
    "\n",
    "        # 掩码矩阵,在pytorch中，有float类型和bool类型\n",
    "        # 填充掩码\n",
    "        if key_padding_mask is not None:\n",
    "           # 扩展与主力scores具有相同的形状\n",
    "            mask = key_padding_mask.view(batch_szie,1,1,-1)\n",
    "            if mask.dtype == torch.bool :\n",
    "               scores = scores.masked_fill(mask,float('-inf'))\n",
    "            else:\n",
    "               scores += mask.to(scores.dtype)\n",
    "        # 注意力掩码 \n",
    "        if atten_mask is not None:\n",
    "            # 省去对齐形状，因为可以自动广播\n",
    "            # mask = key_padding_mask(1,1,tgt_seq_len,src_seq_len)\n",
    "            if atten_mask.dtype == torch.bool:\n",
    "                scores = scores.masked_fill(atten_mask,float('-inf'))\n",
    "            else:\n",
    "                scores += atten_mask.to(scores.dtype)\n",
    "\n",
    "\n",
    "        atten_weigths = torch.softmax(scores,dim=-1)\n",
    "        atten_weigths = self.dropout(atten_weigths)\n",
    "\n",
    "        # 计算注意力输出\n",
    "        atten_output = torch.matmul(atten_weigths,v)\n",
    "\n",
    "        # 合并多头\n",
    "        atten_output = atten_output.transpose(1,2).contiguous().view(batch_szie,-1,self.d_model)\n",
    "        # 线性变换输出\n",
    "        output = self.out_proj(atten_output)\n",
    "        return output,atten_weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状 torch.Size([1, 3, 10])\n",
      "注意力权重的形状 torch.Size([1, 2, 3, 4])\n",
      "注意力权重 tensor([[[[0.1402, 0.5501, 0.0000, 0.0000],\n",
      "          [0.5057, 0.0000, 0.3098, 0.0000],\n",
      "          [0.5500, 0.2913, 0.2698, 0.0000]],\n",
      "\n",
      "         [[0.3939, 0.3612, 0.3561, 0.0000],\n",
      "          [0.3050, 0.5067, 0.2994, 0.0000],\n",
      "          [0.4453, 0.3484, 0.3174, 0.0000]]]], grad_fn=<MulBackward0>)\n",
      "因果掩码 tensor([[0., -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf],\n",
      "        [0., 0., 0., -inf]])\n",
      "加因果掩码的注意力权重 tensor([[[[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.7012, 0.4099, 0.0000, 0.0000],\n",
      "          [0.5500, 0.2913, 0.2698, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4175, 0.6937, 0.0000, 0.0000],\n",
      "          [0.4453, 0.0000, 0.0000, 0.0000]]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    batch_size = 1\n",
    "    d_model = 10\n",
    "    src_seq_len = 4\n",
    "    tgt_seq_len = 3\n",
    "    num_layer = 2\n",
    "\n",
    "    query = torch.randn(batch_size,tgt_seq_len,d_model)\n",
    "    keys = torch.randn(batch_size,src_seq_len,d_model)\n",
    "    values = torch.randn(batch_size,src_seq_len,d_model)\n",
    "    key_padding_mask = torch.zeros(batch_size,src_seq_len,dtype = torch.bool)\n",
    "    key_padding_mask[:,-1] = True\n",
    "    attn = MultiHeadAttention(d_model,num_layer)\n",
    "    output,atten_weigth = attn(query,keys,values,key_padding_mask=key_padding_mask)\n",
    "    print('输出形状',output.shape)\n",
    "    print('注意力权重的形状',atten_weigth.shape)\n",
    "    print('注意力权重',atten_weigth)\n",
    "\n",
    "    # 生产上三角矩阵\n",
    "    cansal_mask = torch.triu(torch.full((tgt_seq_len,src_seq_len),float('-inf')),diagonal = 1)\n",
    "    print('因果掩码',cansal_mask)\n",
    "    _,atten_weigths = attn(query,keys,values,atten_mask=cansal_mask,)\n",
    "    print('加因果掩码的注意力权重',atten_weigths)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., inf, inf, inf],\n",
       "        [0., 0., inf, inf],\n",
       "        [0., 0., 0., inf]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cansal_mask = torch.triu(torch.full((3,4),float('inf')),diagonal=1)\n",
    "cansal_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add&norm\n",
    "\n",
    "LayerNormalization:层归一化，更多用于序列处理  \n",
    "batchNormalization:批归一化，更多用于图像处理  \n",
    "\n",
    "作用：  \n",
    "1、稳定训练过程  \n",
    "2、提高模型泛化能力  \n",
    "3、减轻内部协变量便宜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add & normalization\n",
    "\n",
    "class layernorm(nn.Module):\n",
    "    def __init__(self,normalized_shape,eps = 1e-5):\n",
    "        '''\n",
    "        parameter\n",
    "        ---------------\n",
    "        normalized_shape:int or tuple\n",
    "            需要归一化的特征维度，例如，d_model或（seq_length,d_model)\n",
    "        eps :float\n",
    "            一个很小的值，防止分母为0\n",
    "        '''\n",
    "        super().__init__()\n",
    "        if isinstance(normalized_shape,int):\n",
    "            normalized_shape = (normalized_shape, )\n",
    "        self.normalized_shape = tuple(normalized_shape)\n",
    "        self.eps = eps\n",
    "        # 初始化可学习的参数\n",
    "        self.gamma = nn.Parameter(torch.ones(*self.normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(*self.normalized_shape))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # 确定需要归一化的维度（最后len(normalized_shape)个维度）。\n",
    "        dims = list(range(-len(self.normalized_shape),0))\n",
    "        # 沿着特征维度，计算均值、标准差\n",
    "        # mean,var,shape = (batch_szie,seq_len,1) [假设，normalized_shape长度为1]\n",
    "        mean = x.mean(dim = dims,keepdim = True)\n",
    "        # unbiase,是否进行无偏差估计。（x_i - x^）/(n)为有偏差，（n-1）为无偏差\n",
    "        var = x.var(dim = dims,keepdim = True , unbiased = False)\n",
    "\n",
    "        # x_normalized shape = (batch_size,seq_len,d_model)\n",
    "        x_normalized = (x-mean)/torch.sqrt(var+self.eps)\n",
    "\n",
    "        return self.gamma * x_normalized + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[179., 155., 158., 110., 110.],\n",
      "         [126., 188., 100., 101., 103.],\n",
      "         [180., 175., 102., 158., 174.]],\n",
      "\n",
      "        [[137., 126., 134., 103., 107.],\n",
      "         [124., 100., 198., 107., 191.],\n",
      "         [108., 123., 123., 164., 101.]]])\n",
      "tensor([[[ 1.3205,  0.4546,  0.5628, -1.1690, -1.1690],\n",
      "         [ 0.0714,  1.9166, -0.7024, -0.6726, -0.6131],\n",
      "         [ 0.7692,  0.5960, -1.9334,  0.0069,  0.5613]],\n",
      "\n",
      "        [[ 1.1205,  0.3304,  0.9050, -1.3216, -1.0343],\n",
      "         [-0.4759, -1.0470,  1.2850, -0.8805,  1.1184],\n",
      "         [-0.7232, -0.0366, -0.0366,  1.8399, -1.0435]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    batch_size = 2\n",
    "    seq_len = 3\n",
    "    d_model = 5\n",
    "\n",
    "    x = torch.randint(100,200,size=(batch_size,seq_len,d_model)).float()\n",
    "    print(x)\n",
    "\n",
    "    ln = layernorm(d_model)\n",
    "    print(ln(x))\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add\n",
    "class addNorm(nn.Module):\n",
    "    '''残差链接，层归一化'''\n",
    "    def __init__(self,d_model,p=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = layernorm(d_model)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self,x,sublayer_output):\n",
    "        '''\n",
    "        parameter:\n",
    "        --------------\n",
    "        x ：toech.tensor shape = (batch_size,seq_len,d_model)\n",
    "            上一个子层的输入(自注意力层、前馈神经网络层)\n",
    "        sublayer_output ：torch.tensor shape = (batch_size,seq_len,d_model)\n",
    "            上一个子层的输出\n",
    "        '''\n",
    "        # add\n",
    "        output = x + self.dropout(sublayer_output)\n",
    "        # norm\n",
    "        output = self.norm(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前馈神经网络层（feed forward）\n",
    "\n",
    "ffn(x) = relu(xW1 + b1)W2 + b2  \n",
    "x:输入矩阵\n",
    "w1和w2:线性变换权重矩阵，w1.shape =（d_model,dff）,w2.shape = (dff,d_model),要对原始数据进行升维和降维操作，因此，dff = 4*d_model  \n",
    "b1,b2:偏执，（1，dff）,(1,d_model)  \n",
    "\n",
    "提神模型的表达能力，通过引入更高维度的中间维度，可以捕捉到输入的更多复杂特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self,d_model,dff,p=0.1):\n",
    "        '''\n",
    "        parameter\n",
    "        --------------\n",
    "        d_model : int\n",
    "\n",
    "        '''\n",
    "        super().__init__()\n",
    "        if dff is None:\n",
    "            dff = 4*d_model\n",
    "\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model,dff),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(p),\n",
    "                                 nn.Linear(dff,d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.ffn(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编码器-解码器注意力子层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
